{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D, InputLayer, ZeroPadding2D, GlobalAvgPool2D, Reshape, Softmax\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This tutorial attempts to achieve transfer learning on incompatible image sizes with MobileNet.\n",
    "#  Why do this, when we could just load a different (compatible) data set, or use a different model?\n",
    "#  Because!  Well, this is a tutorial, and it illustrates some practical issues implementing transfer learning.\n",
    "#  We will resize a small batch of images to show how pre-trained weights affect fitting and prediction.\n",
    "#  The unbiased and biased MobileNet objects will have the same layer architecture except for the weights.\n",
    "\n",
    "#  Table of Contents:\n",
    "#  \n",
    "#  1.  Data Processing\n",
    "#  2.  Create MobileNet Conv. Net. Models\n",
    "#  3.  Prediction and Scoring\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: (32,32,3) Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#  Check shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Convince yourself that the labels are consistent with the data.  \n",
    "#  See https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "print(y_train[4999])   \n",
    "plt.imshow(x_train[4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Pretrained weights only exist for certain shapes, which is why we get an error with smaller image sizes.\n",
    "#  One option to deal with the fact that MobileNet does not like (32,32,3) shape is to resize the images.\n",
    "#  NOTE:  On my machine, resizing the entire training data set would take, according to my precise calculations, a very long time.\n",
    "#  I will only be resizing and training on 5,000 (about 50 minutes for me), creating a 2 gig file.  \n",
    "#  You will have to do this on your own, I will not include the resized file on github (although it is possible to host large files).\n",
    "#  Watch your memory usage.\n",
    "\n",
    "#  Base case.\n",
    "resized_train_data = []\n",
    "resized_train_data = np.reshape(np.append(resized_train_data, resize(x_train[0],(128,128,3))),(128,128,3))\n",
    "\n",
    "#  Change range to 50000 if you want to do the entire set.  NOT RECOMMENDED\n",
    "for row in range(5000):\n",
    "    if row > 0:\n",
    "        resized = resize(x_train[row],(128,128,3))\n",
    "        resized_train_data = np.reshape(np.append(resized_train_data, resized),((row+1),128,128,3))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check\n",
    "plt.imshow(resized_train_data[4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Also resize test data\n",
    "resized_test_data = []\n",
    "resized_test_data = np.reshape(np.append(resized_test_data, resize(x_test[0],(128,128,3))),(128,128,3))\n",
    "\n",
    "#  Change range to 10000 if you want to resize entire test data.  NR\n",
    "for row in range(1000):\n",
    "    if row > 0:\n",
    "        resized = resize(x_test[row],(128,128,3))\n",
    "        resized_test_data = np.reshape(np.append(resized_test_data, resized),((row+1),128,128,3))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized_test_data[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Since resizing takes time, you might want to save.  (Also perhaps for resized_test_data, but I omit these details.)\n",
    "#np.save('resized_5k_images.npy',resized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resized_images = np.load('resized_5k_images.npy')\n",
    "\n",
    "#  Or, alternately if you saved as .npz\n",
    "with np.load('/Users/Beebs/Desktop/resized_5k_images.npz') as data:\n",
    "    train_images = data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_images = train_images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Also for test data (which may have a different name if you are loading them.)\n",
    "resized_test_images = resized_test_data / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Category Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encode labels.\n",
    "y_train_encoded = np_utils.to_categorical(y_train)\n",
    "y_test_encoded = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Grab relevant number of labels for resized batch.\n",
    "fivek_labels = y_test_encoded[:5000]\n",
    "fivek_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Create MobileNet Conv. Net. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Unbiased Conv. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  With the resized image shapes, we can use them directlly with MobileNet.  \n",
    "#  Otherwise, it would throw an error that the input_shape is too small.\n",
    "#  However, I am using a dataset of 5000, which does not seem to be enough to increase accuracy above chance.\n",
    "#  This illustrates one of the uses of transfer learning:  when data sets are too small to properly train.\n",
    "\n",
    "fresh = MobileNet(dropout=0,input_shape=(128,128,3),include_top =True, weights=None,classes=10)\n",
    "fresh.summary()\n",
    "fresh.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Fitting takes a long time.  \n",
    "#  If you want to interrupt the cell, click left outside the text field (or hit Esc) and type 'ii'.\n",
    "history_fresh = fresh.fit(resized_train_images,fivek_labels,epochs = 10,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_fresh.history['loss'])\n",
    "#plt.plot(history_fresh.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Biased Conv. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Since we are doing a more complex model, we use the functional API Model() class from keras.\n",
    "#  We manually describe the inputs and outputs.\n",
    "#  For this MobileNet object, we will need to chop off the classification layers when \n",
    "#  using pre-trained weights.    \n",
    "    \n",
    "#  Create tensor object.\n",
    "inputs = Input(shape = (128,128,3)) \n",
    "\n",
    "#  \n",
    "trained_model = MobileNet(dropout=0,input_shape = (128,128,3),include_top = False, weights='imagenet', input_tensor = inputs)\n",
    "\n",
    "#  Freeze: keep pre-trained weights as they are.\n",
    "#  Check number of trainable parameters in summary after freezing layers.\n",
    "for layer in trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#  We can just plug the biased model into a layer.\n",
    "#x = trained_model(inputs)\n",
    "\n",
    "#  Then, if we want we can copy as close as possible the \n",
    "#  structure of the layers removed by include_top=False.\n",
    "\n",
    "x = GlobalAvgPool2D(data_format='channels_last')(trained_model.output)\n",
    "x = Reshape((1,1,-1))(x)\n",
    "x = Dropout(rate=0.001)(x)\n",
    "x = Conv2D(filters=10,kernel_size=(1,1))(x)\n",
    "x = Activation(activation = 'softmax')(x)\n",
    "predictions = Reshape((-1,))(x)\n",
    "\n",
    "\n",
    "transfer_model = Model(inputs = inputs,outputs = predictions)\n",
    "transfer_model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#  Notice fitting the pre-trained model is much quicker, each epoch takes much less time.\n",
    "#  It also begins increasing accuracy with the small sample set I am using (5000).\n",
    "history_transfer = transfer_model.fit(resized_train_images,fivek_labels,epochs=10,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_transfer.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_transfer.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compare with plots from unbiased network.\n",
    "plt.plot(history_fresh.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_fresh.history['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Prediction and Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Supress scientific notation for easier comparison.\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#  Predict a class and look at an example to compare between biased and unbiased.\n",
    "#  What do you expect the comparison to show?\n",
    "unbiased_prediction = fresh.predict(resized_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_prediction[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Note: predict gives us probabilities like predict_proba for other models.  Check that they sum to one.\n",
    "sum(unbiased_prediction[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_prediction = transfer_model.predict(resized_test_images)\n",
    "biased_prediction[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  True label:\n",
    "y_test_encoded[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Unbiased\n",
    "#  Brier score, lower is better: smaller distance between prediction and true label.  \n",
    "#  Try looking at prediction scores before and after training, and after different amounts of training.\n",
    "unbiased_diff = y_test_encoded[:1000] - unbiased_prediction\n",
    "score_u = np.sum((1/1000)*(np.power(unbiased_diff,2)),axis=1)\n",
    "\n",
    "#  Overall score for 1000 test examples.\n",
    "sum(score_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Biased\n",
    "biased_diff = y_test_encoded[:1000] - biased_prediction  \n",
    "score_b = np.sum((1/1000)*(np.power(biased_diff,2)),axis=1)\n",
    "\n",
    "sum(score_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
